<style>
    table {
        width: 100%;
    }
</style>

| Design | Use case |
| --- | --- |
| <details><summary><b>Sobol sequence</b></summary>A Sobol sequence is a type of low-discrepancy sequence used to generate quasi-random numbers. It is particularly effective for uniformly sampling high-dimensional spaces, making it ideal for applications in numerical integration and optimization. The sequence is designed to fill the parameter space more evenly than pseudo-random sequences, reducing the risk of clustering and ensuring better coverage. This is especially useful when the number of experiments is fixed, and you need a representative sampling of the parameter space. Sobol sequences are deterministic, meaning the same sequence can be reproduced exactly, which is beneficial for reproducibility in experiments.</details> | Pseudo-random uniform paving of the parameter space with fixed number of experiments. **OK with multi-level categorical factors**. |
| <details><summary><b>Full factorial design</b></summary>A full factorial experimental design is ideal for systematically exploring all possible combinations of factors at different levels. This means that for $k$ factors, each having $n$ levels, one needs to perform $n^k$ experimental runs. **This design is useful when the number of factors and levels is small, and when interactions between factors are expected to be important**. It is especially useful when the goal is to understand how multiple factors interact with each other and influence the outcome of an experiment. This design provides comprehensive data, allowing for the analysis of both individual effects and interactions between factors. However, it can become resource-intensive as the number of factors increases due to the exponential growth in the number of experiments needed.</details> | Small number of factors and levels. **OK with multi-level categorical factors**. |
| <details><summary><b>Fractional factorial design</b></summary>A fractional factorial design is a reduced version of a full factorial design. The design is orthogonal, meaning that the main effects are uncorrelated with each other. The design is also confounded, meaning that some main effects are aliased with interactions. **The design is particularly useful when the number of factors is large, and when interactions between factors are not expected to be important.** By using fewer runs, fractional designs sacrifice some detailed information, particularly interactions between factors, but are effective in capturing the main effects. This trade-off is acceptable when interactions between factors are assumed to be less critical. The resolution $r$ of a fractional factorial design refers to the ability to distinguish between main effects and interaction effects. Typically, the number of experimental runs becomes $2^{k-r}$.</details> | Large number of factors, interactions not expected to be important. **OK with multi-level categorical factors**. |
| <details><summary><b>Definitive screening design</b></summary>A **Definitive Screening Design (DSD)** is a type of experimental design used in statistics to efficiently identify important factors and their effects in a process or system. It was introduced by Bradley Jones and Christopher Nachtsheim in 2011 as a more efficient and versatile screening tool compared to traditional screening designs, such as fractional factorial or Plackett-Burman designs. The primary purpose of a DSD is to screen factors (independent variables) and identify the most significant ones that influence the response (dependent variable) in an experiment. DSDs are designed to handle experiments where there may be a **large number of factors, but only a few are likely to be influential.** DSDs are structured to avoid the confounding of main effects with two-factor interactions. This means that DSDs allow for the estimation of main effects independently, and the effects of interactions between factors are not confounded with them. Each factor in a DSD is typically tested at three levels: low (-1), middle (0), and high (+1). This allows for the detection of non-linear effects, which is a key advantage over traditional two-level screening designs. DSDs are highly efficient, requiring fewer experimental runs than traditional designs while still providing valuable information about the main effects and interactions. The number of experimental runs is often equal to $2k + 1$, where $k$ is the number of factors, making them much smaller in size for initial screening experiments. DSDs can detect both main effects and interactions, as well as quadratic effects (non-linear relationships between factors and responses). This is a significant improvement over many other screening designs, which focus only on main effects. | Many factors, but few are expected to have a large effect / Non-linear relationships or factor interactions might be present / The experimenter wants a screening design that is both efficient in terms of the number of runs and capable of detecting complex relationships between factors. |
| <details><summary><b>Latin hypercube design</b></summary>A Latin hypercube design (LHD) is a statistical method used for sampling across multiple dimensions in an efficient and balanced way. It divides the range of each variable into equal intervals and ensures that each interval is sampled once, creating a well-distributed set of experimental points. **This design is particularly useful in experiments with many factors and when interactions between factors are not expected to be important**, as it requires fewer runs than a full factorial design while still providing good coverage of the variable space, making it valuable for complex simulations or optimization studies.It is only available for **continuous factors**.</details>| Large number of factors, interactions not expected to be important. **Continuous factors only**. |
|<details><summary><b>Randomized Latin hypercube design</b></summary>A Randomized Latin hypercube design is a statistical method used for efficiently sampling large, multidimensional spaces in experiments. It ensures that the sampling is spread evenly across all variables, reducing the chance of clustering. The design randomizes the selection of points while maintaining balanced coverage of each factor's range. It's particularly useful in situations where you have **many variables**, and you want to optimize sampling efficiency with fewer experimental runs compared to full factorial designs. This is commonly used in simulations or modeling complex systems. It is only available for **continuous factors**.</details> | Large number of factors, interactions not expected to be important. **Continuous factors only**. Space filling. |
| <details><summary><b> Optimal design</b></summary>An optimal design is an experimental design that maximizes the amount of information obtained from the experiment while minimizing resources like time or costs. It is tailored to specific goals, such as estimating model parameters with high precision or detecting significant effects efficiently. Unlike standard designs, optimal designs are generated algorithmically based on the specific model and constraints of the experiment, making them flexible and adaptable to different experimental conditions. Common types include D-optimal, A-optimal, and G-optimal designs.**These designs are particularly useful when the number of factors is large, and when interactions between factors are not expected to be important.**</details> | Large number of factors. |
| <details><summary><b> Plackett-Burman design</b></summary> A Plackett-Burman design is a type of experimental design used for **screening a large number of factors** to identify the most influential ones with a minimal number of experiments. It is an efficient **two-level design** (with high and low settings for each factor) that focuses on main effects, ignoring interactions between factors. This makes it particularly useful in the early stages of experimentation when the goal is to quickly determine which factors significantly affect the outcome.</details> | Screening a large number of 2-levels factors. |
| <details><summary><b> Box-Behnken design</b></summary>A Box-Behnken design is a response surface methodology used for optimization. It's designed to explore the relationships between multiple factors and their interactions. This design only includes combinations where all factors are at their midpoints, highs, or lows, but it avoids extreme combinations, making it more efficient and reducing the risk of extreme conditions that could yield invalid results. **It's commonly used when experimenting with three or more factors** and is effective for **developing quadratic models** without requiring a full factorial design.</details> | 3 or more factors, non linear dependency. |
| <details><summary><b> Central composite design</b></summary> A Central Composite Design (CCD) is an advanced design of experiments method used to build quadratic models for response surface methodology. It's useful for optimizing processes by exploring the relationships between factors and their responses. CCD combines a factorial design with center points and additional "star" points to allow for better estimation of curvature. This design enables efficient estimation of linear, interaction, and quadratic effects, making it ideal for fine-tuning processes or conditions in experiments.</details> | Quadratic models. |
